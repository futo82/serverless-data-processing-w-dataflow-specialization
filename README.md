# Serverless Data Processing with Dataflow Specialization

This repository was created to serve as reference and contains the lab assignments I completed for this specialization. The lecture notes, diagrams, and labs were provided by the [Serverless Data Processing with Dataflow Specialization](https://www.coursera.org/specializations/serverless-data-processing-with-dataflow). 

## About the Specialization

It is becoming harder and harder to maintain a technology stack that can keep up with the growing demands of a data-driven business. Every Big Data practitioner is familiar with the three Vâ€™s of Big Data: volume, velocity, and variety. What if there was a scale-proof technology that was designed to meet these demands?

Enter Google Cloud Dataflow. Google Cloud Dataflow simplifies data processing by unifying batch & stream processing and providing a serverless experience that allows users to focus on analytics, not infrastructure. This specialization is intended for customers & partners that are looking to further their understanding of Dataflow to advance their data processing applications.

## Courses

### Course 1 - Serverless Data Processing with Datflow: Foundations

This course explains how Apache Beam and Dataflow work together to meet your data processing needs without the risk of vendor lock-in.

- Start with a refresher of what Apache Beam is and its relationship with Dataflow
- Talk about Apache Beam vision and the benefits of the Beam Portability framework. The Beam Portability framework achieves the vision that a developer can use their favorite programming language with their preferred execution backend.
- Show how Dataflow allows you to separate compute and storage while saving money, and how identity, access, and management tools interact with your Dataflow pipelines.
- Look at how to implement the right security model for your use case on Dataflow.

### Course 2 - Serverless Data Processing with Dataflow: Develop Pipelines

This course covers how you convert our business logic into data processing applications that can run on Dataflow.

- Review Apache Beam concepts.
- Discuss processing streaming data using windows, watermarks and triggers.
- Cover options for sources and sinks in your pipelines, schemas to express your structured data, and how to do stateful transformations using State and Timer APIs.
- Review best practices that help maximize your pipeline performance.
- Introduce SQL and Dataframes to represent your business logic in Beam and how to iteratively develop pipelines using Beam notebooks.

### Course 3 - Serverless Data Processing with Dataflow: Operations

This course reviews the most important lessons for operating a data application on Dataflow, including monitoring, troubleshooting, testing, and reliability.

- Introduce the components of the Dataflow operational model. We will examine tools and techniques for troubleshooting and optimizing pipeline performance.
- Review testing, deployment, and reliability best practices for Dataflow pipelines.
- Review of Templates, which makes it easy to scale Dataflow pipelines to organizations with hundreds of users.
